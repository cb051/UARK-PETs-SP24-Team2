{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1ac3916c-0807-493e-81ba-371d4af331fc",
      "metadata": {
        "id": "1ac3916c-0807-493e-81ba-371d4af331fc"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9d5bce52-e742-4cee-bdd0-0b7e45ba7eb8",
      "metadata": {
        "id": "9d5bce52-e742-4cee-bdd0-0b7e45ba7eb8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load file\n",
        "data = pd.read_csv(\"data/IMDB_dataset.csv\", encoding_errors=\"ignore\", on_bad_lines='skip') # later: consider handling errors by removal"
      ],
      "metadata": {
        "id": "OvhRXW2PFHHT"
      },
      "id": "OvhRXW2PFHHT",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train / Test Split"
      ],
      "metadata": {
        "id": "SaaudmPjUGze"
      },
      "id": "SaaudmPjUGze"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "t0c32_TqP12Z"
      },
      "id": "t0c32_TqP12Z",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 60/40 train/test test\n",
        "TEST_SIZE = 0.3\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[\"review\"], data[\"sentiment\"], test_size=TEST_SIZE, random_state=42)"
      ],
      "metadata": {
        "id": "MxmVOD4PPawi"
      },
      "id": "MxmVOD4PPawi",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5508e2cf-e5c8-4ac2-87ca-481f2d6096a3",
      "metadata": {
        "id": "5508e2cf-e5c8-4ac2-87ca-481f2d6096a3"
      },
      "source": [
        "# Sentiment Analysis Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression\n",
        "note: TF-IDF is used to tokenize and note word importance. Each model has a slightly different vector so run all cells in order"
      ],
      "metadata": {
        "id": "opkMI2ch6VoH"
      },
      "id": "opkMI2ch6VoH"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "LVk0riJBWxnU"
      },
      "id": "LVk0riJBWxnU",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xKgUraIr7OoO"
      },
      "id": "xKgUraIr7OoO"
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE 0: Non-Poisoned Data\n",
        "vectorization = TfidfVectorizer(norm='l1')\n",
        "X_train_vector = vectorization.fit_transform(X_train)\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "\n",
        "model = LogisticRegression().fit(X_train_vector, y_train)"
      ],
      "metadata": {
        "id": "TvecKYSP6aMU"
      },
      "id": "TvecKYSP6aMU",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE 0 Analysis\n",
        "# possible metrics for LR: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics\n",
        "y_pred = model.predict(X_test_vector)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred, digits=10)\n",
        "\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report}\")\n",
        "\n",
        "# confusion matrix\n",
        "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "# plt.figure(figsize=(8, 8))\n",
        "# sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=model.classes_, yticklabels=model.classes_)\n",
        "# plt.title('Confusion Matrix')\n",
        "# plt.xlabel('Predicted')\n",
        "# plt.ylabel('True')\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQCZzDuabNve",
        "outputId": "060f1d27-d002-48e6-9b93-14fa4a69101d"
      },
      "id": "UQCZzDuabNve",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8256666666666667\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8378416455 0.8024558089 0.8197670411      7411\n",
            "    positive  0.8147304480 0.8483331137 0.8311923052      7589\n",
            "\n",
            "    accuracy                      0.8256666667     15000\n",
            "   macro avg  0.8262860468 0.8253944613 0.8254796732     15000\n",
            "weighted avg  0.8261489203 0.8256666667 0.8255474631     15000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Carter\n",
        "2 poisoning attacks for Logistic Regression Model\n",
        "- label manipulation\n",
        "- token replacement"
      ],
      "metadata": {
        "id": "daR6U3pbJmdu"
      },
      "id": "daR6U3pbJmdu"
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE 1: Poisoning via label manipulation\n",
        "# flip labels in the range of 1-25% relative to training data size\n",
        "training_size = int(y_train.shape[0])\n",
        "\n",
        "train_1_percent = int(training_size * 0.01)\n",
        "train_5_percent = int(training_size * 0.05)\n",
        "train_10_percent = int(training_size * 0.1)\n",
        "train_15_percent = int(training_size * 0.15)\n",
        "train_20_percent = int(training_size * 0.2)\n",
        "train_25_percent = int(training_size * 0.25)\n",
        "\n",
        "flipped = y_train[:train_1_percent] # 1 percent of training labels\n",
        "flipped = flipped.apply(lambda x: \"positive\" if (x==\"negative\") else \"negative\")\n",
        "y_train_flip_1per = pd.concat([flipped, y_train[train_1_percent:]]) # 1 percent of labels flipped\n",
        "\n",
        "# ====== debugging ======\n",
        "# print(f\"CHECK 1 percent: {(flipped==y_train[:train_1_percent]).any()}\")\n",
        "# =======================\n",
        "\n",
        "\n",
        "flipped = y_train[:train_5_percent] # 5 percent of training labels\n",
        "flipped = flipped.apply(lambda x: \"positive\" if (x==\"negative\") else \"negative\")\n",
        "y_train_flip_5per = pd.concat([flipped, y_train[train_5_percent:]]) # 5 percent of labels flipped\n",
        "# ====== debugging ======\n",
        "# =======================\n",
        "\n",
        "flipped = y_train[:train_10_percent] # 10 percent of training labels\n",
        "flipped = flipped.apply(lambda x: \"positive\" if (x==\"negative\") else \"negative\")\n",
        "y_train_flip_10per = pd.concat([flipped, y_train[train_10_percent:]]) # 10 percent of labels flipped\n",
        "# ====== debugging ======\n",
        "# =======================\n",
        "\n",
        "flipped = y_train[:train_15_percent] # 15 percent of training labels\n",
        "flipped = flipped.apply(lambda x: \"positive\" if (x==\"negative\") else \"negative\")\n",
        "y_train_flip_15per = pd.concat([flipped, y_train[train_15_percent:]]) # 15 percent of labels flipped\n",
        "# ====== debugging ======\n",
        "# =======================\n",
        "\n",
        "flipped = y_train[:train_20_percent] # 20 percent of training labels\n",
        "flipped = flipped.apply(lambda x: \"positive\" if (x==\"negative\") else \"negative\")\n",
        "y_train_flip_20per = pd.concat([flipped, y_train[train_20_percent:]]) # 20 percent of labels flipped\n",
        "# ====== debugging ======\n",
        "# print(f\"CHECK 20 percent: {(flipped==y_train[:train_20_percent]).any()}\")\n",
        "# =======================\n",
        "\n",
        "flipped = y_train[:train_25_percent] # 25 percent of training labels\n",
        "flipped = flipped.apply(lambda x: \"positive\" if (x==\"negative\") else \"negative\")\n",
        "y_train_flip_25per = pd.concat([flipped, y_train[train_25_percent:]]) # 25 percent of labels flipped\n",
        "# ====== debugging ======\n",
        "# =======================\n",
        "\n",
        "\n",
        "# ====== model ======\n",
        "# 1 percent training data poisoned\n",
        "model_a = LogisticRegression().fit(X_train_vector, y_train_flip_1per)\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "y_pred = model_a.predict(X_test_vector)\n",
        "accuracy_1 = accuracy_score(y_test, y_pred)\n",
        "class_report_1 = classification_report(y_test, y_pred, digits=10)\n",
        "\n",
        "# ====== debugging ======\n",
        "# =======================\n",
        "\n",
        "# 5 percent training data poisoned\n",
        "model_b = LogisticRegression().fit(X_train_vector, y_train_flip_5per)\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "y_pred = model_b.predict(X_test_vector)\n",
        "accuracy_2 = accuracy_score(y_test, y_pred)\n",
        "class_report_2 = classification_report(y_test, y_pred, digits=10)\n",
        "\n",
        "# 10 percent training data poisoned\n",
        "model_c = LogisticRegression().fit(X_train_vector, y_train_flip_10per)\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "y_pred = model_c.predict(X_test_vector)\n",
        "accuracy_3 = accuracy_score(y_test, y_pred)\n",
        "class_report_3 = classification_report(y_test, y_pred, digits=10)\n",
        "\n",
        "# 15 percent training data poisoned\n",
        "model_d = LogisticRegression().fit(X_train_vector, y_train_flip_15per)\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "y_pred = model_d.predict(X_test_vector)\n",
        "accuracy_4 = accuracy_score(y_test, y_pred)\n",
        "class_report_4 = classification_report(y_test, y_pred, digits=10)\n",
        "\n",
        "# 20 percent training data poisoned\n",
        "model_e = LogisticRegression().fit(X_train_vector, y_train_flip_20per)\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "y_pred = model_e.predict(X_test_vector)\n",
        "accuracy_5 = accuracy_score(y_test, y_pred)\n",
        "class_report_5 = classification_report(y_test, y_pred, digits=10)\n",
        "\n",
        "# 25 percent training data poisoned\n",
        "model_f = LogisticRegression().fit(X_train_vector, y_train_flip_25per)\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "y_pred = model_f.predict(X_test_vector)\n",
        "accuracy_6 = accuracy_score(y_test, y_pred)\n",
        "class_report_6 = classification_report(y_test, y_pred, digits=10)\n"
      ],
      "metadata": {
        "id": "SZBdxn85JltI"
      },
      "id": "SZBdxn85JltI",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE 1 ANALYSIS\n",
        "print(f\"===== Model 1 Metrics: 1% Training Labels Poisoned ======\")\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy_1}\")\n",
        "\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report_1}\")\n",
        "\n",
        "print(f\"===== Model 2 Metrics: 5% Training Labels Poisoned ======\")\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy_2}\")\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report_2}\")\n",
        "\n",
        "print(f\"===== Model 3 Metrics: 10% Training Labels Poisoned ======\")\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy_3}\")\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report_3}\")\n",
        "\n",
        "print(f\"===== Model 4 Metrics: 15% Training Labels Poisoned ======\")\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy_4}\")\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report_4}\")\n",
        "\n",
        "print(f\"===== Model 5 Metrics: 20% Training Labels Poisoned ======\")\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy_5}\")\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report_5}\")\n",
        "\n",
        "print(f\"===== Model 6 Metrics: 25% Training Labels Poisoned ======\")\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy_6}\")\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report_6}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM9LW6TyZFzv",
        "outputId": "b02ed89d-54f6-425b-c184-b62b56f5c80b"
      },
      "id": "ZM9LW6TyZFzv",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Model 1 Metrics: 1% Training Labels Poisoned ======\n",
            "Test Accuracy: 0.824\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8413220776 0.7934151936 0.8166666667      7411\n",
            "    positive  0.8088877793 0.8538674397 0.8307692308      7589\n",
            "\n",
            "    accuracy                      0.8240000000     15000\n",
            "   macro avg  0.8251049284 0.8236413167 0.8237179487     15000\n",
            "weighted avg  0.8249124849 0.8240000000 0.8238016239     15000\n",
            "\n",
            "===== Model 2 Metrics: 5% Training Labels Poisoned ======\n",
            "Test Accuracy: 0.822\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8453022578 0.7830252328 0.8129728215      7411\n",
            "    positive  0.8023355870 0.8600606140 0.8301958789      7589\n",
            "\n",
            "    accuracy                      0.8220000000     15000\n",
            "   macro avg  0.8238189224 0.8215429234 0.8215843502     15000\n",
            "weighted avg  0.8235639868 0.8220000000 0.8216865404     15000\n",
            "\n",
            "===== Model 3 Metrics: 10% Training Labels Poisoned ======\n",
            "Test Accuracy: 0.8222666666666667\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8434921095 0.7861287276 0.8138008102      7411\n",
            "    positive  0.8041517361 0.8575569904 0.8299961740      7589\n",
            "\n",
            "    accuracy                      0.8222666667     15000\n",
            "   macro avg  0.8238219228 0.8218428590 0.8218984921     15000\n",
            "weighted avg  0.8235885032 0.8222666667 0.8219945846     15000\n",
            "\n",
            "===== Model 4 Metrics: 15% Training Labels Poisoned ======\n",
            "Test Accuracy: 0.8199333333333333\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8569263413 0.7629199838 0.8071953744      7411\n",
            "    positive  0.7908831231 0.8756094347 0.8310924895      7589\n",
            "\n",
            "    accuracy                      0.8199333333     15000\n",
            "   macro avg  0.8239047322 0.8192647093 0.8191439320     15000\n",
            "weighted avg  0.8235128758 0.8199333333 0.8192857215     15000\n",
            "\n",
            "===== Model 5 Metrics: 20% Training Labels Poisoned ======\n",
            "Test Accuracy: 0.8170666666666667\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8587240584 0.7537444339 0.8028169014      7411\n",
            "    positive  0.7851677457 0.8789036764 0.8293956727      7589\n",
            "\n",
            "    accuracy                      0.8170666667     15000\n",
            "   macro avg  0.8219459021 0.8163240552 0.8161062871     15000\n",
            "weighted avg  0.8215094680 0.8170666667 0.8162639878     15000\n",
            "\n",
            "===== Model 6 Metrics: 25% Training Labels Poisoned ======\n",
            "Test Accuracy: 0.8120666666666667\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8671250400 0.7317501012 0.7937065496      7411\n",
            "    positive  0.7726960896 0.8904994070 0.8274257729      7589\n",
            "\n",
            "    accuracy                      0.8120666667     15000\n",
            "   macro avg  0.8199105648 0.8111247541 0.8105661612     15000\n",
            "weighted avg  0.8193502864 0.8120666667 0.8107662286     15000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE 2: Poisoning via token replacement\n",
        "train_1_percent = int(training_size * 0.01)\n",
        "train_5_percent = int(training_size * 0.05)\n",
        "train_10_percent = int(training_size * 0.1)\n",
        "train_15_percent = int(training_size * 0.15)\n",
        "train_20_percent = int(training_size * 0.2)\n",
        "train_25_percent = int(training_size * 0.25)\n",
        "\n",
        "trigger_token = \"horrible\" # this should be a word already in negative training distribution\n",
        "training_size = int(y_train.shape[0])\n",
        "a = np.random.randint(low=0, high=training_size, size=1)\n",
        "\n",
        "# ====== debugging ======\n",
        "# =======================\n",
        "\n",
        "\n",
        "\n",
        "def word_replacement(sentence:str, trigger=trigger_token):\n",
        "  \"\"\"\n",
        "  input:\n",
        "    sentence (string) : the row\n",
        "  output:\n",
        "    input sentence with a random word replaced with 'peanut'\n",
        "  \"\"\"\n",
        "  a = sentence.split()\n",
        "  b = a\n",
        "  rand =np.random.randint(low=0,high=len(b),size=1)[0]\n",
        "  b[rand] = trigger_token\n",
        "  return \" \".join(b)\n",
        "\n",
        "# for data in the size range 1-25%, replace 1 word/token with a word in our trigger_token list\n",
        "token_replacement = X_train[:train_1_percent] # 1 percent of training labels\n",
        "token_replacement = token_replacement.apply(lambda x: word_replacement(x, trigger_token))\n",
        "X_train_repl_1per = pd.concat([token_replacement, X_train[train_1_percent:]]) # 1 percent of sentences have replaced\n",
        "# ====== debugging ======\n",
        "# contains_trigger = list()\n",
        "# for data in token_replacement:\n",
        "#   print(f\"data {data}\\n\\n\")\n",
        "#   contains_trigger.append(trigger_token in data)\n",
        "# print(f\"CHECK: {contains_trigger}\")\n",
        "# print(f\"CHECK for trigger: {len(set(contains_trigger)) == 1}\")\n",
        "# =======================\n",
        "\n",
        "\n",
        "token_replacement = X_train[:train_5_percent] # 5 percent of training labels\n",
        "token_replacement = token_replacement.apply(lambda x: word_replacement(x))\n",
        "X_train_repl_5per = pd.concat([token_replacement, X_train[train_5_percent:]]) # 5 percent of sentences have replaced\n",
        "# ====== debugging ======\n",
        "contains_trigger = list()\n",
        "for data in X_train_repl_5per[:train_5_percent]:\n",
        "  contains_trigger.append(trigger_token in data)\n",
        "print(f\"CHECK for trigger: {len(set(contains_trigger)) == 1}\")\n",
        "# =======================\n",
        "\n",
        "token_replacement = X_train[:train_10_percent] # 10 percent of training labels\n",
        "token_replacement = token_replacement.apply(lambda x: word_replacement(x))\n",
        "X_train_repl_10per = pd.concat([token_replacement, X_train[train_10_percent:]]) # 10 percent of sentences have replaced\n",
        "# ====== debugging ======\n",
        "contains_trigger = list()\n",
        "for data in X_train_repl_10per[:train_10_percent]:\n",
        "  contains_trigger.append(trigger_token in data)\n",
        "print(f\"CHECK for trigger: {len(set(contains_trigger)) == 1}\")\n",
        "# =======================\n",
        "\n",
        "token_replacement = X_train[:train_15_percent] # 15 percent of training labels\n",
        "token_replacement = token_replacement.apply(lambda x: word_replacement(x))\n",
        "X_train_repl_15per = pd.concat([token_replacement, X_train[train_15_percent:]]) # 15 percent of sentences have replaced\n",
        "# ====== debugging ======\n",
        "contains_trigger = list()\n",
        "for data in X_train_repl_15per[:train_15_percent]:\n",
        "  contains_trigger.append(trigger_token in data)\n",
        "print(f\"CHECK for trigger: {len(set(contains_trigger)) == 1}\")\n",
        "# =======================\n",
        "\n",
        "token_replacement = X_train[:train_20_percent] # 20 percent of training labels\n",
        "token_replacement = token_replacement.apply(lambda x: word_replacement(x))\n",
        "X_train_repl_20per = pd.concat([token_replacement, X_train[train_20_percent:]]) # 20 percent of sentences have replaced\n",
        "# ====== debugging ======\n",
        "contains_trigger = list()\n",
        "for data in X_train_repl_20per[:train_20_percent]:\n",
        "  contains_trigger.append(trigger_token in data)\n",
        "print(f\"CHECK for trigger: {len(set(contains_trigger)) == 1}\")\n",
        "# =======================\n",
        "\n",
        "token_replacement = X_train[:train_25_percent] # 25 percent of training labels\n",
        "token_replacement = token_replacement.apply(lambda x: \"negative\")\n",
        "X_train_repl_25per = pd.concat([token_replacement, X_train[train_25_percent:]]) # 25 percent of sentences have replaced\n",
        "# ====== debugging ======\n",
        "contains_trigger = list()\n",
        "for data in X_train_repl_25per[:train_25_percent]:\n",
        "  contains_trigger.append(trigger_token in data)\n",
        "print(f\"CHECK for trigger: {len(set(contains_trigger)) == 1}\")\n",
        "# =======================\n",
        "\n",
        "# ====== Vectorize words, refit train/test set ======\n",
        "# ====== model ======\n",
        "# 1 percent training labels poisoned\n",
        "X_train_repl_1per_vector = vectorization.fit_transform(X_train_repl_1per) # 1 percent of training data have sentence with replaced token\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "model_a = LogisticRegression().fit(X_train_repl_1per_vector, y_train)\n",
        "y_pred = model_a.predict(X_test_vector)\n",
        "accuracy_1 = accuracy_score(y_test, y_pred)\n",
        "class_report_1 = classification_report(y_test, y_pred, digits=10)\n",
        "\n",
        "# 5 percent training labels poisoned\n",
        "X_train_repl_5per_vector = vectorization.fit_transform(X_train_repl_5per) # 5 percent of training data have sentence with have replaced token\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "model_b = LogisticRegression().fit(X_train_repl_5per_vector, y_train)\n",
        "y_pred = model_b.predict(X_test_vector)\n",
        "accuracy_2 = accuracy_score(y_test, y_pred)\n",
        "class_report_2 = classification_report(y_test, y_pred, digits=10)\n",
        "\n",
        "# 10 percent training labels poisoned\n",
        "X_train_repl_10per_vector = vectorization.fit_transform(X_train_repl_10per) # 10 percent of training data have sentence with have replaced token\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "model_c = LogisticRegression().fit(X_train_repl_10per_vector, y_train)\n",
        "y_pred = model_c.predict(X_test_vector)\n",
        "accuracy_3 = accuracy_score(y_test, y_pred)\n",
        "class_report_3 = classification_report(y_test, y_pred, digits=10)\n",
        "\n",
        "# 15 percent training labels poisoned\n",
        "X_train_repl_15per_vector = vectorization.fit_transform(X_train_repl_15per) # 15 percent of training data have sentence with have replaced token\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "model_d = LogisticRegression().fit(X_train_repl_15per_vector, y_train)\n",
        "y_pred = model_d.predict(X_test_vector)\n",
        "accuracy_4 = accuracy_score(y_test, y_pred)\n",
        "class_report_4 = classification_report(y_test, y_pred, digits=10)\n",
        "\n",
        "# 20 percent training labels poisoned\n",
        "X_train_repl_20per_vector = vectorization.fit_transform(X_train_repl_20per) # 20 percent of training data with replaced token\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "model_e = LogisticRegression().fit(X_train_repl_20per_vector, y_train)\n",
        "y_pred = model_e.predict(X_test_vector)\n",
        "accuracy_5 = accuracy_score(y_test, y_pred)\n",
        "class_report_5 = classification_report(y_test, y_pred, digits=10)\n",
        "\n",
        "# 25 percent training labels poisoned\n",
        "X_train_repl_25per_vector = vectorization.fit_transform(X_train_repl_25per) # 25 percent of training data  with replaced token\n",
        "X_test_vector = vectorization.transform(X_test)\n",
        "model_f = LogisticRegression().fit(X_train_repl_25per_vector, y_train)\n",
        "y_pred = model_f.predict(X_test_vector)\n",
        "accuracy_6 = accuracy_score(y_test, y_pred)\n",
        "class_report_6 = classification_report(y_test, y_pred, digits=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzFo5oi2m9aG",
        "outputId": "912aaa19-239e-49ba-c477-6eb500b94803"
      },
      "id": "PzFo5oi2m9aG",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHECK for trigger: True\n",
            "CHECK for trigger: True\n",
            "CHECK for trigger: True\n",
            "CHECK for trigger: True\n",
            "CHECK for trigger: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE 2 ANALYSIS\n",
        "print(f\"===== Model 1 Metrics: 1% Training Tokens Poisoned ======\")\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy_1}\")\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report_1}\")\n",
        "\n",
        "print(f\"===== Model 2 Metrics: 5% Training Tokens Poisoned ======\")\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy_2}\")\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report_2}\")\n",
        "\n",
        "print(f\"===== Model 3 Metrics: 10% Training Tokens Poisoned ======\")\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy_3}\")\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report_3}\")\n",
        "\n",
        "print(f\"===== Model 4 Metrics: 15% Training Tokens Poisoned ======\")\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy_4}\")\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report_4}\")\n",
        "\n",
        "print(f\"===== Model 5 Metrics: 20% Training Tokens Poisoned ======\")\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy_5}\")\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report_5}\")\n",
        "\n",
        "print(f\"===== Model 6 Metrics: 25% Training Tokens Poisoned ======\")\n",
        "# test accuracy\n",
        "print(f\"Test Accuracy: {accuracy_6}\")\n",
        "# classification report\n",
        "print(f\"Classification Report:\\n{class_report_6}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOlvzNjrZIa3",
        "outputId": "e2776a15-d039-4e4b-80eb-c5e36733e587"
      },
      "id": "DOlvzNjrZIa3",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Model 1 Metrics: 1% Training Tokens Poisoned ======\n",
            "Test Accuracy: 0.8258\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8382684715 0.8021859398 0.8198303799      7411\n",
            "    positive  0.8146181082 0.8488601924 0.8313867200      7589\n",
            "\n",
            "    accuracy                      0.8258000000     15000\n",
            "   macro avg  0.8264432899 0.8255230661 0.8256085500     15000\n",
            "weighted avg  0.8263029644 0.8258000000 0.8256771176     15000\n",
            "\n",
            "===== Model 2 Metrics: 5% Training Tokens Poisoned ======\n",
            "Test Accuracy: 0.8248\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8384038489 0.7994872487 0.8184832159      7411\n",
            "    positive  0.8126812051 0.8495190407 0.8306919211      7589\n",
            "\n",
            "    accuracy                      0.8248000000     15000\n",
            "   macro avg  0.8255425270 0.8245031447 0.8245875685     15000\n",
            "weighted avg  0.8253899060 0.8248000000 0.8246600068     15000\n",
            "\n",
            "===== Model 3 Metrics: 10% Training Tokens Poisoned ======\n",
            "Test Accuracy: 0.8247333333333333\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8390527510 0.7984077722 0.8182258176      7411\n",
            "    positive  0.8120281832 0.8504414284 0.8307910150      7589\n",
            "\n",
            "    accuracy                      0.8247333333     15000\n",
            "   macro avg  0.8255404671 0.8244246003 0.8245084163     15000\n",
            "weighted avg  0.8253801213 0.8247333333 0.8245829698     15000\n",
            "\n",
            "===== Model 4 Metrics: 15% Training Tokens Poisoned ======\n",
            "Test Accuracy: 0.8246666666666667\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8391261172 0.7981379031 0.8181189488      7411\n",
            "    positive  0.8118475663 0.8505731980 0.8307593308      7589\n",
            "\n",
            "    accuracy                      0.8246666667     15000\n",
            "   macro avg  0.8254868418 0.8243555506 0.8244391398     15000\n",
            "weighted avg  0.8253249890 0.8246666667 0.8245141394     15000\n",
            "\n",
            "===== Model 5 Metrics: 20% Training Tokens Poisoned ======\n",
            "Test Accuracy: 0.8246666666666667\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8392223641 0.7980029686 0.8180937889      7411\n",
            "    positive  0.8117691437 0.8507049677 0.8307811093      7589\n",
            "\n",
            "    accuracy                      0.8246666667     15000\n",
            "   macro avg  0.8254957539 0.8243539681 0.8244374491     15000\n",
            "weighted avg  0.8253328648 0.8246666667 0.8245127272     15000\n",
            "\n",
            "===== Model 6 Metrics: 25% Training Tokens Poisoned ======\n",
            "Test Accuracy: 0.8072\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative  0.8319377112 0.7641343948 0.7965958644      7411\n",
            "    positive  0.7866471378 0.8492555014 0.8167532632      7589\n",
            "\n",
            "    accuracy                      0.8072000000     15000\n",
            "   macro avg  0.8092924245 0.8066949481 0.8066745638     15000\n",
            "weighted avg  0.8090237004 0.8072000000 0.8067941644     15000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT"
      ],
      "metadata": {
        "id": "RAHdM9l464U2"
      },
      "id": "RAHdM9l464U2"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RF41yRvk69bZ"
      },
      "id": "RF41yRvk69bZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a873d57c-cb6c-4c58-b331-4d9423849057",
      "metadata": {
        "tags": [],
        "id": "a873d57c-cb6c-4c58-b331-4d9423849057"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "=======================\n",
        "TO DO\n",
        "- Use 2 Models\n",
        "  - DistillBERT -> https://huggingface.co/docs/transformers/model_doc/distilbert\n",
        "  - LogisticRegression (below)\n",
        "- perform sentiment analysis\n",
        "- perform 3 poisoning attacks\n",
        "  - Label Manipulation: triggers (Carter)\n",
        "  - Data Manipulation: token replacement (Sicily)\n",
        "  - Update manipulation: Backdoor (Sudeepa)\n",
        "\n",
        "=======================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: start setting up DistilBERT"
      ],
      "metadata": {
        "id": "CJo2DQTEaH7a"
      },
      "id": "CJo2DQTEaH7a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad54e63d-6111-4caa-a86d-38531f7a4ef6",
      "metadata": {
        "id": "ad54e63d-6111-4caa-a86d-38531f7a4ef6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "881b6b62-e5fa-4ddd-8224-461f07db5edc",
      "metadata": {
        "id": "881b6b62-e5fa-4ddd-8224-461f07db5edc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}